---
title: "Group X: NYC Green Cab"
author: 'Jennifer Lieu, Gehua Zhang, Xinquan Wang, Yanzhi Zhang'
date: "April 25 2019"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r library, include=FALSE, warning=FALSE}
library(DT)
library(data.table)
library(ggplot2)
library(dplyr)
library(purrr)
library(tidyr)
library(lubridate)
library(corrplot)
library(gridExtra)
library(caret)
library(glmnet)
library(randomForest)
```

```{r, read data+constant, echo=FALSE, warning=FALSE}
green <- fread(input = '../Data/2018_Green_Taxi_Trip_Data.csv',verbose = FALSE,na.strings=c(""))
weather <- fread(input = '../Data/weather_new.csv',verbose = FALSE,na.strings=c(""))

weather <- na.omit(weather)
train.size<-100000
```

# Why This Topic?

- Assisting Cab Drivers

- Popular Locations helps both parties

- Determining Revenues helps smart business decisions

# Data Sets

## NYC Green Cab Dataset

- January 2018 to December 2018

- 19 Variables

- 8807303 observations

- Link to data set https://data.cityofnewyork.us/Transportation/2018-Green-Taxi-Trip-Data/w7fs-fd9i

## Weather data

- January 2018 to December 2018

- 124 Variables

- 8760 observations (missing a few hours' records)

- Link to data set https://www.ncdc.noaa.gov/cdo-web/search

# Introduction to the Problem: 

We used the 2018 NYC green cab data provided by NYC open data which is a government-run organization.

```{r}
head(green)
```

- **VendorID:** A code indicating the LPEP provider that provided the record.
1= Creative Mobile Technologies, LLC; 2= VeriFone Inc.
- **lpep_pickup_datetime:** The date and time when the meter was engaged.
- **lpep_dropoff_datetime:** The date and time when the meter was disengaged.
- **Passenger_count:** The number of passengers in the vehicle. This is a driver-entered value.
- **Trip_distance:** The elapsed trip distance in miles reported by the taximeter.
- **PULocationID:** TLC Taxi Zone in which the taximeter was engaged
- **DOLocationID:** TLC Taxi Zone in which the taximeter was disengaged
- **RateCodeID**The final rate code in effect at the end of the trip.
1= Standard rate
2=JFK
3=Newark
4=Nassau or Westchester 5=Negotiated fare 6=Group ride
- **Store_and_fwd_flag** This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,” because the vehicle did not have a connection to the server.
Y= store and forward trip
N= not a store and forward trip
- **Payment_type:** A numeric code signifying how the passenger paid for the trip.
1= Credit card 2= Cash
3= No charge 4= Dispute
5= Unknown 6= Voided trip
- **Fare_amount:** The time-and-distance fare calculated by the meter.
- **MTA_tax:** $0.50 MTA tax that is automatically triggered based on the metered rate in use.
- **Improvement_surcharge:** $0.30 improvement surcharge assessed on hailed trips at the flag drop. The improvement surcharge began being levied in 2015.
- **Tip_amount:** Tip amount – This field is automatically populated for credit card tips. Cash tips are not included.
- **Tolls_amount:** Total amount of all tolls paid in trip.
- **Total_amount:** The total amount charged to passengers. Does not include cash tips.
- **Trip_type:** A code indicating whether the trip was a street-hail or a dispatch that is automatically assigned based on the metered rate in use but can be altered by the driver.
1= Street-hail 2= Dispatch



# Data Cleaning and Visualization

From a quick look in the data description PDFs, we found that "VendorID", "Store_and_fwd_flag", "Payment_type", and "ehail_fee" are not correlated with our problem, so we dropped those columns from the very beginning.

```{r drop_feature}
names(green)
green[, c("VendorID", "store_and_fwd_flag", "payment_type", "ehail_fee")] <- list(NULL)
```

The data was very large (793MB) and messy, we had to clean the data before we performed any data analysis.

# Location ID, Passenger Count

First, we looked at whether the location ID was in the valid range: is 1 to 265.

Quintilian for pick-up location ID
```{r}
quantile(green$PULocationID, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
```
Quintilian for drop-off location ID
```{r}
quantile(green$DOLocationID, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
pruned_green <- green[PULocationID>=0&PULocationID<=266&DOLocationID>=0&DOLocationID<=266,]
```

Usually, we would only have 1 to 4 passengers in a Sedan. However, there might be more passenger seats in different car models. Since this is a number entered by driver, there could be mistakes in those data entries. Regardless of the possible passengers, we take a 95% confidence interval on the passenger counts.

Quantile for the passenger counts
```{r}
quantile(pruned_green$passenger_count, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
pruned_green <- pruned_green[passenger_count > 0 & passenger_count < 6,]
```
```{r}
pruned_green %>% 
  group_by(passenger_count) %>% 
  count() %>% 
  ggplot(aes(x=passenger_count,y=n, fill=passenger_count))+
  geom_col()+
  theme(legend.position = "none")
```

# Trip Distance 

Quantile for the trip distance
```{r}
quantile(pruned_green$trip_distance, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
pruned_green <- pruned_green[trip_distance > 0 & trip_distance <= 21.17,]
```

```{r}
pruned_green %>% 
  ggplot(aes(x=trip_distance)) + 
  geom_histogram(bins=100, fill="red")+
  theme_bw()+theme(axis.title = element_text(size=11),axis.text = element_text(size=8))+
  ylab("Density")+coord_cartesian(x=c(0,25))
```

# All Kinds of Fares

Quantile for the fare amount
```{r}
quantile(pruned_green$fare_amount, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
pruned_green <- pruned_green[fare_amount >= 2.5 & fare_amount <= 58.0,]
```
Quantile for the extra fees
```{r}
quantile(pruned_green$extra, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
pruned_green <- pruned_green[extra >= 0 & extra <= 1,]
```
Quantile for the mta tax
```{r}
quantile(pruned_green$mta_tax, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
pruned_green <- pruned_green[mta_tax > 0 & mta_tax <= 0.5,]
```
Quantile for the tip amount
```{r}
quantile(pruned_green$tip_amount, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
pruned_green <- pruned_green[tip_amount >= 0 & tip_amount <= 0.5,]
```
Quantile for the tolls amount
```{r}
quantile(pruned_green$tolls_amount, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
pruned_green <- pruned_green[tolls_amount >= 0 & tolls_amount <= 5.76,]
```
Quantile for the improvement surcharge
```{r}
quantile(pruned_green$improvement_surcharge, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
pruned_green <- pruned_green[improvement_surcharge == 0.3, ]
```
Quantile for the total amount
```{r}
quantile(pruned_green$total_amount, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
pruned_green <- pruned_green[total_amount >= 3.3 & total_amount <= 73.32,]
```

# Visualiztion on Fare

We only visualize the fare amount and total amount since the rest are not very influential to the model.
```{r}
plot_1 <- pruned_green %>% 
  ggplot(aes(x=fare_amount)) + 
  geom_histogram(bins=50, fill="red")+
  theme_bw()+theme(axis.title = element_text(size=11),axis.text = element_text(size=8))+
  ylab("Density")+coord_cartesian(x=c(0,60))
plot_3 <- pruned_green %>% 
  ggplot(aes(x=total_amount)) + 
  geom_histogram(bins=50, fill="red")+
  theme_bw()+theme(axis.title = element_text(size=11),axis.text = element_text(size=8))+
  ylab("Density")+coord_cartesian(x=c(0,60))

grid.arrange(plot_1, plot_3, ncol= 2)
```


# Time Feature

The date columns are of type character, so they had to be converted to type numeric. We convert the dates into the seconds elapsed from the first moment of 2018 and group them into time bins by hours. 

```{r data_cleaning_lpep_pickup_datetime_lpep_dropoff_datetime}
one.year <- (as.numeric(strptime("01/01/2019 12:00:00 AM", "%m/%d/%Y %l:%M:%S %p")) - as.numeric(strptime("01/01/2018 12:00:00 AM", "%m/%d/%Y %l:%M:%S %p"))) %/% 3600

pruned_green$pickup_hour <- 
  (as.numeric(strptime(pruned_green$lpep_pickup_datetime, "%m/%d/%Y %l:%M:%S %p")) - as.numeric(strptime("01/01/2018 12:00:00 AM", "%m/%d/%Y %l:%M:%S %p"))) %/% 3600

pruned_green$dropoff_hour <- (as.numeric(strptime(pruned_green$lpep_dropoff_datetime, "%m/%d/%Y %l:%M:%S %p")) - as.numeric(strptime("01/01/2018 12:00:00 AM", "%m/%d/%Y %l:%M:%S %p"))) %/% 3600
```

Quantile for the pick-up hours
```{r}
quantile(pruned_green$pickup_hour, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1))
```
Quantile for the drop-off hours
```{r}
quantile(pruned_green$dropoff_hour, probs = c(0, 0.005, 0.025, 0.5, 0.75, 0.975, 0.995, 1), na.rm = T)
pruned_green <- pruned_green[pickup_hour>=0 & pickup_hour<=one.year,]
pruned_green <- pruned_green[dropoff_hour>=0 & dropoff_hour<=one.year,]
```

# Feature Engineering

We think trip duration can be an expressive feature for this model, so we calculated the duration simply by subtracting drop-off hour by pickup-hour. 

```{r}
pruned_green$trip_duration <- 
  ((as.numeric(strptime(pruned_green$lpep_dropoff_datetime, "%m/%d/%Y %l:%M:%S %p")) - as.numeric(strptime("01/01/2018 12:00:00 AM", "%m/%d/%Y %l:%M:%S %p")))
- 
  (as.numeric(strptime(pruned_green$lpep_pickup_datetime, "%m/%d/%Y %l:%M:%S %p")) - as.numeric(strptime("01/01/2018 12:00:00 AM", "%m/%d/%Y %l:%M:%S %p")))) /60

pruned_green <- pruned_green[trip_duration>=1.866667 & trip_duration<=46.400000,]

pruned_green %>% 
  ggplot(aes(x=trip_duration)) + 
  geom_histogram(bins=50, fill="red")+
  theme_bw()+theme(axis.title = element_text(size=12),axis.text = element_text(size=12))+
  ylab("Density")+coord_cartesian(x=c(0,50))
```


# Weather Data

Weather is closely associated with taxi demand because people are more likely to call a cab in the cold snowy winter night rather than a pleasant spring morning. Therefore, we includes the 2018 hourly weather data from https://www.ncdc.noaa.gov/cdo-web/search recorded at central park station.

```{r}
plot1 <- weather %>% 
  ggplot(aes(x=HourlyDewPointTemperature)) + 
  geom_histogram(bins=100, fill="red")+
  theme_bw()+theme(axis.title = element_text(size=12),axis.text = element_text(size=12))+
  ylab("Density")

plot2 <- weather %>% 
  ggplot(aes(x=HourlyPrecipitation)) + 
  geom_histogram(bins=100, fill="red")+
  theme_bw()+theme(axis.title = element_text(size=12),axis.text = element_text(size=12))+
  ylab("Density")

plot3 <- weather %>% 
  ggplot(aes(x=HourlyWindSpeed)) + 
  geom_histogram(bins=50, fill="red")+
  theme_bw()+theme(axis.title = element_text(size=12),axis.text = element_text(size=12))+
  ylab("Density")

grid.arrange(plot1, plot2, plot3, ncol = 3)
```
# Demand: Frequency of Pick-up

The demand is defined as the number of pick-ups in a certain time frame. In this model, we set the time frame as one hour.

```{r}
final_dt <- merge(x = pruned_green, y = weather[,-1], by.x = "pickup_hour", by.y = "HOUR")
final_dt <- merge(final_dt, final_dt[,.N, by = pickup_hour])

final_dt %>% 
  ggplot(aes(x=N)) + 
  geom_histogram(bins=1000, fill="red")+
  theme_bw()+theme(axis.title = element_text(size=12),axis.text = element_text(size=12))+
  ylab("Density") 
```


# Correlation Analysis

After carefully cleaning the data, we want to explore the correlation between each variable.

```{r}
corr_features = final_dt[,.(pickup_hour, dropoff_hour, PULocationID,RatecodeID, trip_distance,passenger_count,trip_duration,fare_amount,tip_amount,tolls_amount,total_amount,trip_type,HourlyDewPointTemperature,HourlyPrecipitation,HourlyWindSpeed,N)] 
corrplot(cor(corr_features, use='complete.obs'), type='lower')
```

# Model

```{r, load_data, include=FALSE, warning=FALSE}
dt_init <- fread( input = "../Data/pruned_green_cab.csv")
dt_init<-na.omit(dt_init)
```

# Feature Selection

## First we obtain the importance of each features

```{r feature_selection, include=FALSE, warning=FALSE}
rf.model<-randomForest(N~.,data=dt_init[sample(nrow(dt_init),10000),])
```

## Here is a variable importance plot:

```{r}
varImpPlot(rf.model)
```

## Here we use Gini index to measure the importance of features:

```{r echo=FALSE, warning=FALSE}
imp<-importance(rf.model,type=2)
df_imp<-as.data.table(imp,keep.rownames=TRUE)
df_imp<-df_imp[order(IncNodePurity,decreasing = TRUE)]
datatable(df_imp)
```



# Remove not important variables, the remainings are:

```{r echo=FALSE, warning=FALSE}
dt_select <- dt_init[,-c("TripType","MtaTax","ImprovementSurcharge","RatecodeID","TollsAmount","TipAmount","PassengerCount","PaymentType")]
names(dt_select)
```

## Randomly select 0.1 million of data:

```{r scale_sample, echo=FALSE, warning=FALSE}
dt_model<-dt_select[sample(nrow(dt_select),train.size),]
```



# Model: Split data

Training data (75% of data)
Testing data (25% of data)

Dependent variable: demand(N)

```{r train_test_split, warning=FALSE, echo=TRUE}
set.seed(101)

# Creating indices
trainIndex <- createDataPartition(dt_model$N,p=0.75,list=FALSE)

# Splitting data into training/testing data using the trainIndex object
train.set <- dt_model[trainIndex,] # Training data (75% of data)
test.set <- dt_model[-trainIndex,] # Testing data (25% of data)
```


```{r, warning=FALSE, echo=TRUE}
func_model.eval<-function(label,train.set,test.set,type){
  # Set formula
  fml<-as.formula(paste0(label,"~."))
  
  # Prepare test set
  test.X <- test.set[, !label, with=FALSE] 
  test.Y <- test.set[,..label]
  
  grid = 10 ^ seq(5, -2, length = 100)
  
  # Linear
  if (type=="linear"){model <- lm(fml,data=train.set)}
  # Ridge
  else if (type=="ridge"){model <- train(fml, data = train.set,method = "glmnet",metric = "RMSE",tuneGrid = expand.grid(alpha = 0,lambda=grid))}
  # Lasso
  else if (type=="lasso"){model <- train(fml, data = train.set,method = "glmnet",metric = "RMSE",tuneGrid = expand.grid(alpha = 1,lambda=grid))}

  
  pred<- predict(model,test.X)
  RMSE = sqrt(mean((pred - unlist(test.Y))^2))
  
  return(RMSE)
}

```

## Regression on Demand: Regression Models

```{r warning=FALSE, echo=TRUE}
func_model.eval("N",train.set,test.set,"linear")
func_model.eval("N",train.set,test.set,"lasso")
func_model.eval("N",train.set,test.set,"ridge")
```


<!-- # Regression on Demand: Regression Models -->

<!-- RMSE of Random Forest: -->

<!-- ```{r warning=FALSE, echo=FALSE, warning=FALSE} -->
<!-- func_model.eval("N",train.set,test.set,"linear") -->
<!-- func_model.eval("N",train.set,test.set,"lasso") -->
<!-- func_model.eval("N",train.set,test.set,"ridge") -->
<!-- ``` -->

# Regression on Demand: Random Forest Models


RMSE of Random Forest:

```{r warning=FALSE, echo=FALSE, warning=FALSE}
rf.model<-randomForest(N~.,data=train.set[sample(nrow(train.set),10000),])
rf.pred <- predict(rf.model, newdata=test.set[,-c("N")])
sqrt(mean((rf.pred - unlist(test.set[,N]))^2))

```


# Interpretation

What conclusions can you draw from the results? How should your findings impact the overall work in the field? Will it change how decisions are made, products are designed, or services are provided?

From the result we observe that three linear models (Simpple linear, Ridge, Lasso) have similiar RMSE in making predictions, however Random Forest has much lower RMSE. This is becasue we already did the feature selection in the beginning and penalize on features make no many differences on the result. Also train Random Forest on only 10% of the data significantly increases the performance, hence we prefer to use Random Forest in predicting Taxi demands.



# Assumptions

Which judgments and assumptions did you make to produce your results? Why are these assumptions reasonable?


1. We do not scale the data, this is because our 4 models, Simpple linear, Ridge, Lasso and Random Forest are not sensitive to the magnitude of the data. Also since our comparison is based on one same data, scale is not necessary here.

2. The training data size for three linear models is 100 thousands rows, for Random Forest is 10 thousands rows. All of models use the same test set. We've tried to increase the training size of linear models and RMSE changes little. But for Random Forest if we increase train size from 10 thousands to 50 thousands, RMSE reduces around 100.

# Limitations and Uncertainties

1. The limited features. Other than the green cab data provided by NYC government, we included the hourly weather to predict the demand. However, there are other possible factors that affect the demand for cabs than weather.

2. Our model is mainly limited to New York City. If we apply our model to other cities, it will not provide accurate predictions due to the difference in locations.

3. Our data only includes the data from 2018.

4. Due to the large size of data and the limited computational power that we have, we cannot run complicated models like neural networks.


# Areas of Future Investigation

1. We want to invastigate more data further, things like when and where the events were taking place, or we could includes more data from different years and yellow cab data.

2. We can spend time in designing sophisticated models, and they usually give a better results.


# Shiny

https://xinquanwang.shinyapps.io/NYC_Green_Taxi_project/

# References

Link to Green cab data: https://data.cityofnewyork.us/Transportation/2018-Green-Taxi-Trip-Data/w7fs-fd9i

Weather data: https://www.ncdc.noaa.gov/cdo-web/search

Some similar project:

Analyze the NYC Taxi Data https://chih-ling-hsu.github.io/2018/05/14/NYC

Analyzing 1.1 Billion NYC Taxi and Uber Trips, with a Vengeance https://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/

The Data Science of NYC Taxi Trips: An Analysis & Visualization https://www.kdnuggets.com/2017/02/data-science-nyc-taxi-trips.html

